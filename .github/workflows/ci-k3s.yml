name: CI/CD Pipeline (GitHub-Hosted with WireGuard)

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main, develop ]
    types: [ opened, synchronize, reopened ]

env:
  PYTHON_VERSION: '3.11'
  K3S_CLUSTER_IP: '10.204.201.1'
  WG_PRIVATE_KEY: ${{ secrets.WG_PRIVATE_KEY }}
  WG_PEER_PUBLIC_KEY: ${{ secrets.WG_PEER_PUBLIC_KEY }}
  WG_ENDPOINT: ${{ secrets.WG_ENDPOINT }}

jobs:
  test:
    name: Run Tests
    runs-on: ubuntu-latest

    steps:
    - uses: actions/checkout@v4

    - name: Setup WireGuard tunnel to K3s cluster
      run: |
        echo "üîê Setting up WireGuard tunnel to K3s cluster at ${{ env.K3S_CLUSTER_IP }}..."

        # Install WireGuard
        sudo apt-get update
        sudo apt-get install -y wireguard

        # Create WireGuard configuration
        cat <<EOF | sudo tee /etc/wireguard/wg0.conf
        [Interface]
        PrivateKey = ${{ env.WG_PRIVATE_KEY }}
        Address = 10.0.0.2/32
        DNS = 10.204.201.1

        [Peer]
        PublicKey = ${{ env.WG_PEER_PUBLIC_KEY }}
        Endpoint = ${{ env.WG_ENDPOINT }}
        AllowedIPs = 10.0.0.1/32, 10.204.201.0/24
        PersistentKeepalive = 25
        EOF

        # Start WireGuard interface
        sudo wg-quick up wg0

        # Test connection to K3s cluster
        ping -c 3 ${{ env.K3S_CLUSTER_IP }} || {
          echo "‚ùå Cannot reach K3s cluster through WireGuard"
          sudo wg-quick down wg0
          exit 1
        }

        echo "‚úÖ WireGuard tunnel established"
        sudo wg show

    - name: Configure kubectl for K3s cluster
      run: |
        echo "‚öôÔ∏è Configuring kubectl for remote K3s cluster..."

        # Create kubeconfig for remote K3s cluster
        mkdir -p $HOME/.kube

        # Use kubeconfig from secret
        echo "${{ secrets.K3S_KUBECONFIG }}" | base64 -d > $HOME/.kube/config

        # Replace localhost with cluster IP
        sed -i 's/127.0.0.1/${{ env.K3S_CLUSTER_IP }}/g' $HOME/.kube/config
        chmod 600 $HOME/.kube/config

        echo "‚úÖ kubectl configured"
        kubectl cluster-info

    - name: Deploy MongoDB for tests
      run: |
        echo "üóÑÔ∏è Deploying ephemeral MongoDB pod in K3s..."

        # Create unique namespace for this test run
        TEST_NAMESPACE="ci-${{ github.run_id }}"
        kubectl create namespace $TEST_NAMESPACE --dry-run=client -o yaml | kubectl apply -f -

        # Deploy MongoDB pod
        kubectl run mongodb-${{ github.run_id }} \
          --image=mongo:6.0 \
          --port=27017 \
          --namespace=$TEST_NAMESPACE \
          --restart=Never \
          --env="MONGO_INITDB_ROOT_USERNAME=admin" \
          --env="MONGO_INITDB_ROOT_PASSWORD=testpass" \
          --labels="test-run=${{ github.run_id }}"

        # Expose MongoDB service
        kubectl expose pod mongodb-${{ github.run_id }} \
          --port=27017 \
          --namespace=$TEST_NAMESPACE \
          --name=mongodb-svc-${{ github.run_id }}

        # Wait for MongoDB to be ready
        kubectl wait --for=condition=ready pod/mongodb-${{ github.run_id }} \
          --namespace=$TEST_NAMESPACE \
          --timeout=120s

        echo "‚úÖ MongoDB deployed in namespace: $TEST_NAMESPACE"

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}

    - name: Cache dependencies
      uses: actions/cache@v4
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt', '**/pyproject.toml') }}
        restore-keys: |
          ${{ runner.os }}-pip-

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements/base.txt
        pip install -r requirements/test.txt

    - name: Run unit tests
      env:
        MONGODB_URI: mongodb://admin:testpass@mongodb-svc-${{ github.run_id }}.ci-${{ github.run_id }}.svc.cluster.local:27017
        S3_BUCKET_NAME: test-bucket
        AZURE_STORAGE_ACCOUNT: test-account
        GCP_PROJECT_ID: test-project
      run: |
        pytest tests/test_api.py -v --cov=src/backend --cov-report=xml

    - name: Run integration tests
      env:
        MONGODB_URI: mongodb://admin:testpass@mongodb-svc-${{ github.run_id }}.ci-${{ github.run_id }}.svc.cluster.local:27017
      run: |
        pytest tests/test_integration.py -v

    - name: Upload coverage to Codecov
      uses: codecov/codecov-action@v3
      with:
        file: ./coverage.xml
        flags: unittests
        name: codecov-umbrella
        fail_ci_if_error: false

    - name: Teardown WireGuard tunnel
      if: always()
      run: |
        echo "üîê Tearing down WireGuard tunnel..."
        sudo wg-quick down wg0 || true
        echo "‚úÖ WireGuard tunnel stopped"

    - name: Cleanup test resources
      if: always()
      run: |
        TEST_NAMESPACE="ci-${{ github.run_id }}"
        echo "üßπ Cleaning up test resources in namespace: $TEST_NAMESPACE..."

        # Re-establish tunnel for cleanup
        sudo wg-quick up wg0 || true

        # Delete test namespace and all resources
        kubectl delete namespace $TEST_NAMESPACE --wait=false --ignore-not-found=true || true

        # Teardown tunnel again
        sudo wg-quick down wg0 || true
        echo "‚úÖ Cleanup completed"

  lint:
    name: Lint Code
    runs-on: ubuntu-latest

    steps:
    - uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}

    - name: Install linting tools
      run: |
        python -m pip install --upgrade pip
        pip install flake8 black isort mypy

    - name: Run Black
      run: black --check src/ tests/
      continue-on-error: true

    - name: Run isort
      run: isort --check-only src/ tests/
      continue-on-error: true

    - name: Run Flake8
      run: flake8 src/ tests/ --max-line-length=120 --ignore=E203,W503
      continue-on-error: true

    - name: Run MyPy
      run: |
        # Use mypy with pyproject.toml configuration to avoid module path duplication
        mypy src/backend src/speecher --config-file pyproject.toml
      continue-on-error: true

  container-build:
    name: Build Container Images
    runs-on: ubuntu-latest

    steps:
    - uses: actions/checkout@v4

    - name: Set up Docker Buildx
      uses: docker/setup-buildx-action@v3

    - name: Cache Docker layers
      uses: actions/cache@v4
      with:
        path: /tmp/.buildx-cache
        key: ${{ runner.os }}-buildx-${{ github.sha }}
        restore-keys: |
          ${{ runner.os }}-buildx-

    - name: Build Backend Image
      run: |
        echo "üèóÔ∏è Building backend image with Docker..."
        docker build \
          --file=./Dockerfile \
          --tag=speacher-backend:test \
          --cache-from type=local,src=/tmp/.buildx-cache \
          --cache-to type=local,dest=/tmp/.buildx-cache-new,mode=max \
          .

        # Move cache
        rm -rf /tmp/.buildx-cache
        mv /tmp/.buildx-cache-new /tmp/.buildx-cache || true

        echo "‚úÖ Backend image built successfully"
        docker images speacher-backend:test

    - name: Build Frontend Image
      run: |
        if [ -f "docker/react.Dockerfile" ]; then
          echo "üèóÔ∏è Building frontend image with Docker..."
          docker build \
            --file=./docker/react.Dockerfile \
            --tag=speacher-frontend:test \
            --cache-from type=local,src=/tmp/.buildx-cache \
            --cache-to type=local,dest=/tmp/.buildx-cache-new-frontend,mode=max \
            .

          # Move cache
          rm -rf /tmp/.buildx-cache-new-frontend || true

          echo "‚úÖ Frontend image built successfully"
          docker images speacher-frontend:test
        else
          echo "‚ÑπÔ∏è Frontend Dockerfile not found, skipping frontend build"
        fi

    - name: Test with Kubernetes (Full Stack Integration)
      run: |
        echo "üß™ Testing built containers with full K3s integration..."

        # Setup WireGuard for K8s access
        sudo apt-get install -y wireguard
        cat <<EOF | sudo tee /etc/wireguard/wg0.conf
        [Interface]
        PrivateKey = ${{ env.WG_PRIVATE_KEY }}
        Address = 10.0.0.2/32
        [Peer]
        PublicKey = ${{ env.WG_PEER_PUBLIC_KEY }}
        Endpoint = ${{ env.WG_ENDPOINT }}
        AllowedIPs = 10.0.0.1/32, 10.204.201.0/24
        PersistentKeepalive = 25
        EOF
        sudo wg-quick up wg0
        ping -c 2 ${{ env.K3S_CLUSTER_IP }}

        # Setup kubectl
        mkdir -p $HOME/.kube
        echo "${{ secrets.K3S_KUBECONFIG }}" | base64 -d > $HOME/.kube/config
        sed -i 's/127.0.0.1/${{ env.K3S_CLUSTER_IP }}/g' $HOME/.kube/config

        # Deploy test stack using Kubernetes with Docker-built images
        # Note: Images need to be pushed to registry accessible from K3s cluster
        # For now, just verify build succeeds
        echo "‚úÖ Container builds completed successfully"
        echo "‚ö†Ô∏è  Full K8s integration test requires image registry setup"

        sudo wg-quick down wg0 || true

  security:
    name: Security Scan
    runs-on: ubuntu-latest

    steps:
    - uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}

    - name: Install security tools
      run: |
        python -m pip install --upgrade pip
        pip install safety bandit

    - name: Run Safety check
      run: |
        pip install -r requirements/base.txt || true
        safety check || true
      continue-on-error: true

    - name: Run Bandit
      run: bandit -r src/ -f json -o bandit-report.json
      continue-on-error: true

    - name: Container Security Scan with Trivy
      uses: aquasecurity/trivy-action@master
      with:
        image-ref: python:3.11-slim
        format: 'sarif'
        output: 'trivy-results.sarif'
      continue-on-error: true

    - name: Upload security reports
      uses: actions/upload-artifact@v4
      with:
        name: security-reports
        path: |
          bandit-report.json
          trivy-results.sarif
        retention-days: 30

  deploy:
    name: Deploy to Production
    needs: [test, lint, container-build, security]
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/main' && github.event_name == 'push'

    steps:
    - uses: actions/checkout@v4

    - name: Setup WireGuard tunnel to K3s cluster
      run: |
        echo "üîê Setting up WireGuard tunnel for deployment..."

        sudo apt-get install -y wireguard

        cat <<EOF | sudo tee /etc/wireguard/wg0.conf
        [Interface]
        PrivateKey = ${{ env.WG_PRIVATE_KEY }}
        Address = 10.0.0.2/32
        DNS = 10.204.201.1

        [Peer]
        PublicKey = ${{ env.WG_PEER_PUBLIC_KEY }}
        Endpoint = ${{ env.WG_ENDPOINT }}
        AllowedIPs = 10.0.0.1/32, 10.204.201.0/24
        PersistentKeepalive = 25
        EOF

        sudo wg-quick up wg0
        ping -c 3 ${{ env.K3S_CLUSTER_IP }}

        echo "‚úÖ WireGuard tunnel established"

    - name: Configure kubectl for deployment
      run: |
        echo "‚öôÔ∏è Configuring kubectl..."

        mkdir -p $HOME/.kube
        echo "${{ secrets.K3S_KUBECONFIG }}" | base64 -d > $HOME/.kube/config
        sed -i 's/127.0.0.1/${{ env.K3S_CLUSTER_IP }}/g' $HOME/.kube/config
        chmod 600 $HOME/.kube/config

        kubectl cluster-info

    - name: Deploy to K3s Cluster
      run: |
        echo "üöÄ Deploying to K3s cluster..."

        # Apply production manifests
        # kubectl apply -f k8s/production/

        echo "‚úÖ Deployment to K3s completed!"
        echo "This is where you would deploy to production manifests"

    - name: Teardown WireGuard tunnel
      if: always()
      run: |
        sudo wg-quick down wg0 || true
        echo "‚úÖ WireGuard tunnel stopped"
